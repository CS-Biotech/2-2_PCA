{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h6SINw7KfE3"
   },
   "source": [
    "# Week 2: Introduction to Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkSEUe9kIgDc"
   },
   "source": [
    "## Tutorial\n",
    "\n",
    "In this week, we will study **dimensionality reduction**, and more specifically about **principal component analysis (PCA)**.\n",
    "\n",
    "In this module you will learn:\n",
    "\n",
    "1. sketch the direction of the first principal component for some given data in a 2-D space\n",
    "2. explain why it is crucial to standardize the data before applying PCA\n",
    "3. apply PCA in Python using sci-kit learn\n",
    "4. adjust PCA parameters for your specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIwU0nDNIhs3"
   },
   "source": [
    "### Dimensionality Reduction\n",
    "\n",
    "*What is dimensionality reduction?*\n",
    "\n",
    "Dimensionality reduction is a technique to reduce the number of features (characteristics, or input variables, etc.) of a dataset, while preserving the most important information.  \n",
    "\n",
    "*Why would we ever need to reduce the number of features in a dataset?*\n",
    "\n",
    "Let's look at an example. Suppose that we are analyzing a data set of breast cancer histopathology images. We have provided two images below. The left image contains benign breast cancer issues, whereas the right image contains malignant breast cancer tissues. Our goal is to determine whether an image contains benign or malignant breast cancer issues. How would you perform this prediction?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![benign-malignant.jpg](download.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkJo-QZrKPIb"
   },
   "source": [
    "*How do we predict whether an image contains benign or malignant cancer issues?*\n",
    "\n",
    "You might notice that benign and breast cancer tissues have distinct characteristics, such as different ductal structures, nuclear morphologies, cell morphologies, and so on.\n",
    "\n",
    "In a computer, each image is represented as a 2-D grid of pixels. Each pixel provides a tiny piece of information through its color. You can think of the pixels in an image encoding some information about the breast cancer issue.\n",
    "We often consider each pixel of an image as a **feature**. The value of each feature is the color of the pixel converted to a number. (You may already be familiar with the term **features** under another name, \"variables\".)\n",
    "\n",
    "Although each image has many pixels, only some pixels are important for determining whether the breast cancer issue is benign or malignent. We want to figure out and isolate the important pixels. Our ultimate goal is to preserve all the relevant features (pixels) that are required for classification and remove all the rest, ultimately reducing the dimensions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYSQs8sJxBdg"
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkDckhkPxBdg"
   },
   "source": [
    "Analyzing actual breast cancer histopathology images would require a lot of compute power since these images are **extremely** high dimensional (i.e. have a lot of pixels). Instead, we're going to look at the breast cancer dataset from sklearn. This dataset contains features extracted from images of breast cancer biopsies. This data is still high-dimensional, but it is feasible for us to perform the analysis and interpret the results.\n",
    "\n",
    "To begin let's import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dGERppE6Q67Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd # for data manipulation and analysis\n",
    "\n",
    "import matplotlib.pyplot as plt   #  for plotting\n",
    "import matplotlib as mpl    # for configurating the plotting\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# sklearn for machine learning\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c8grzaNhxBdg"
   },
   "outputs": [],
   "source": [
    "# we'll use the breast cancer dataset as an example of high-dimensional biological data\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UuVSoUjxBdh"
   },
   "source": [
    "Features of an actual breast cancer image are pixels of the image. The sklearn dataset loaded provides less features for each breast cancer sample. Let's look at a description of the sklearn breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ydv-Kr3zxBdh"
   },
   "outputs": [],
   "source": [
    "print(data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCh8n46qxBdh"
   },
   "source": [
    "---\n",
    "##### **Q1. How many features does the dataset have? Give 4 examples of the features.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write Answer Here**</span>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3K2zpfXRxBdh"
   },
   "source": [
    "Even though 30 features is much less than the number of pixels in an image, they are difficult to visualize 30 dimensions. Instead, we can use PCA to bring the data down into two dimensions, which we can visualize on a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9GJqHklIpk7"
   },
   "source": [
    "### Explaining Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nBDcat0G2lX"
   },
   "source": [
    "Principal Component Analysis (PCA) aims to preserve the features that \"explain the most variance\". We would therefore expect the principal components that the data is reduced to, to correspond to the biggest \"spread\" or variance in data points. Variance is a measurement of how data points differ from the mean.\n",
    "\n",
    "> A Toy Example\n",
    "\n",
    "Before we attempt to apply PCA to the sklearn breast cancer dataset, let's use a toy example to gain some intuition about PCA. The code below generates a dataset with two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jSCMASlXItZM"
   },
   "outputs": [],
   "source": [
    "# let's generate a toy 2D dataset with a strong linear relationship\n",
    "np.random.seed(42)\n",
    "X_new = np.dot(np.random.rand(2, 2), np.random.randn(2, 200)).T\n",
    "X_new = X_new - X_new.mean(axis=0)  # center data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsX_37GAUdKT"
   },
   "source": [
    "Let's plot the data to visualize the two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ProzdQUKyi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Generated Toy Data')\n",
    "plt.axis('equal')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2Vsh5u0CaTJ"
   },
   "source": [
    "Next, let's explore how to reduce our 2D data to 1D. Imagine we pick a line and \"flatten\" all the data onto it. This process involves projecting each data point onto the line. We aim to understand this by visualizing how data points are moved to the line and measuring the \"error\" of this projection, which is the distance each point has to travel to reach the line. Ideally we want a pick a line that explains the most variance, so the distance these all these points have to move is the shortest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-bN4fAtCu8f"
   },
   "outputs": [],
   "source": [
    "# define a line for projection: y = mx + c\n",
    "# choose m and c to fit your data's general direction...\n",
    "# (you can play around with slope and intercept values this but remember to return to original values before continuing on:\n",
    "# m = 1\n",
    "# c = 0\n",
    "m = 1  # slope\n",
    "c = 0  # intercept\n",
    "\n",
    "# calculate projection points on the line for each X_new point (i.e. the distance each point must travel)\n",
    "X_line = np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 100)\n",
    "Y_line = m * X_line + c\n",
    "X_proj = (m * (X_new[:, 1] - c) + X_new[:, 0]) / (m**2 + 1)\n",
    "Y_proj = (m**2 * X_new[:, 1] + m * X_new[:, 0] + c) / (m**2 + 1)\n",
    "\n",
    "# plot data (same as above!)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5, label='Original Data')\n",
    "# plot the line (line we defined with slope and intercept)\n",
    "plt.plot(X_line, Y_line, color='red', label='Projection Line')\n",
    "# plot projections (our distances!)\n",
    "for i in range(len(X_new)):\n",
    "    plt.plot([X_new[i, 0], X_proj[i]], [X_new[i, 1], Y_proj[i]], 'k--')\n",
    "plt.scatter(X_proj, Y_proj, color='green', label='Projected Points', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Projection onto a Line')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39wxZTCXEkrg"
   },
   "source": [
    "Now, we'll project our original data onto a different line, one that is orthogonal to the first line we used. By doing this, we aim to show that not all projections are equally useful. Specifically, we want to compare the effectiveness of these two lines in capturing the variance of the original data. The idea is to illustrate why one projection might be better than another by looking at how much distance points have to move during the projection, which directly relates to how well the line captures the data's variance.\n",
    "\n",
    "To make this comparison, we'll calculate the average distance points must travel to be projected onto this new line. Then, we can compare this average distance with the one from the first projection to see which line results in a lower average distance, indicating a more efficient capture of the data's variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtF-qvVWEG9q"
   },
   "outputs": [],
   "source": [
    "m = -1 / m  # slope of the new line, orthogonal to the first line\n",
    "c = Y_proj.mean() - m * X_proj.mean()  # adjusting intercept to pass through the mean of projected points\n",
    "\n",
    "# define new projection line\n",
    "X_line_new = np.linspace(X_new[:, 0].min(), X_new[:, 0].max(), 100)\n",
    "Y_line_new = m * X_line_new + c\n",
    "\n",
    "# calculate new projection points\n",
    "X_proj_new = (m * (X_new[:, 1] - c) + X_new[:, 0]) / (m**2 + 1)\n",
    "Y_proj_new = (m**2 * X_new[:, 1] + m * X_new[:, 0] + c) / (m**2 + 1)\n",
    "\n",
    "# calculate distances for the new projection\n",
    "distances_new = np.sqrt((X_new[:, 0] - X_proj_new)**2 + (X_new[:, 1] - Y_proj_new)**2)\n",
    "\n",
    "# new projection\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5, label='Original Data')\n",
    "plt.plot(X_line_new, Y_line_new, color='red', label='Orthogonal Projection Line')\n",
    "for i in range(len(X_new)):\n",
    "    plt.plot([X_new[i, 0], X_proj_new[i]], [X_new[i, 1], Y_proj_new[i]], 'k--')\n",
    "plt.scatter(X_proj_new, Y_proj_new, color='green', label='Projected Points', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Orthogonal Projection onto a Line')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJ7TrASaFoDD"
   },
   "outputs": [],
   "source": [
    "#let's compare the average distances\n",
    "average_distance_original = np.mean(np.sqrt((X_new[:, 0] - X_proj)**2 + (X_new[:, 1] - Y_proj)**2))\n",
    "average_distance_new = np.mean(distances_new)\n",
    "\n",
    "average_distance_original, average_distance_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJCIgXqZFpxE"
   },
   "source": [
    "---\n",
    "##### **Q2: Which direction was better to project the data points?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write Answer Here**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsz-ypXuI8y3"
   },
   "source": [
    "After exploring how projections onto different lines can vary in their effectiveness at capturing data variance, we're now ready to introduce Principal Component Analysis (PCA) into our discussion.\n",
    "\n",
    "Essentially, PCA is a systematic method to identify the **most informative projection directions**â€”those that maximize variance and, hence, capture the data's inherent structure. Unlike our earlier manual explorations with lines, PCA automates this search and quantifies the importance of each direction.\n",
    "\n",
    "In this section, we apply PCA to our toy data to visually understand how PCA identifies principal components. These components are the best lines we can project our data onto if we want to preserve as much information as possible.\n",
    "\n",
    "The principal components show us not just any lines but the ones along which the variance of our data is maximized. This is basically a way to automatically find the optimal projection line, with a data-driven approach.\n",
    "\n",
    "Let's take a look at how PCA works on our toy data. We'll plot the original data points and overlay the principal components as vectors. These vectors represent the directions of maximum variance, with their length indicative of the variance magnitude explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Bxc3hPVcmd"
   },
   "outputs": [],
   "source": [
    "# StandardScaler is used to standardize features by removing the means and adjust to the variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# use StandardScaler to scale the features\n",
    "# We will explain why scaling data is necessary in PCA later\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_new)\n",
    "\n",
    "# apply PCA to toy data\n",
    "pca_2d = PCA(n_components=2)\n",
    "pca_2d.fit(X_scaled)\n",
    "\n",
    "# plot the original toy data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.5)\n",
    "\n",
    "#add the principal components to the plot as direction and magnitude arrows\n",
    "for length, vector in zip(pca_2d.explained_variance_, pca_2d.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    plt.quiver(pca_2d.mean_[0], pca_2d.mean_[1], v[0], v[1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Principal Components on Toy Data')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOMFG74YIIcg"
   },
   "source": [
    "### Explaining Scaling in PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwRSWjK5IJ8S"
   },
   "source": [
    "In our implementation of PCA earlier, prior to applying PCA, we scaled the data, now let's explain why.\n",
    "\n",
    "Scaling is a critical preprocessing step for PCA, especially when your features have different units and scales. This is because PCA looks for directions of maximum variance to identify principal components. Variance is heavily influenced by the scale of the features; If one feature has a much larger scale than others, PCA might misleadingly consider that feature more important, not because of any intrinsic property of the data, but simply due to its larger numeric ranges.\n",
    "\n",
    "Let's look at a simple example. Given a dataset with two features:\n",
    "-  Feature 1: ranges from 0 to 1\n",
    "-  Feature 2: ranges from 0 to 100\n",
    "    \n",
    "Without scaling, PCA can overemphasize the variance along the direction of larger numerical ranges, which is Feature 2. If plotting the PCA, the principal component would lie almost entirely along the axis of Feature 2.\n",
    "Consequently, the variance captured by PCA is overwhelming influenced by Feature 2 and overshadowing any contribution from Feature 1. This leads to bias towards Feature 2.\n",
    "Feature 1 may be equally important, however, PCA ignores Feature 1 because of its range is small and influence on variance is small before scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLgwiWUYJavI"
   },
   "source": [
    "With scaling, PCA gives a more balanced and accurate representation of the data's intrinsic structure, highlighting the true directions that maximize variance. Scaling before applying PCA ensures that all features contribute equally to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJavaGQeRSH3"
   },
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Jiaq_TlxBdj"
   },
   "source": [
    "Now let's go back to the breast cancer dataset that is introduced at the beginning.\n",
    "\n",
    "Let's prepare the data for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VuNUMFCcxBdj"
   },
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gt6zKDakxBdj"
   },
   "outputs": [],
   "source": [
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7WEXRFLxBdj"
   },
   "source": [
    "Starting with standardizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xRpPzplJRPGA"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw79ngo3RY7c"
   },
   "source": [
    "### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t7AH0mAFd1Uv"
   },
   "outputs": [],
   "source": [
    "#we're setting the number of components to 2 to get a 2D visualization by changing value of n_components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR0UsmBuSo2p"
   },
   "outputs": [],
   "source": [
    "#look at the PCA Results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='plasma', edgecolor='k', s=40)\n",
    "plt.title('2D PCA of Breast Cancer Dataset')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(label='Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hO7fAbpRguZS"
   },
   "source": [
    "As we coloured the samples by target ID, we can see that the data appears separable based on the target label. We could draw a line on this graph to separate cancerous and non-cancerous samples (our malignant vs. benign labelled histopathologies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JITdf83ueYop"
   },
   "source": [
    "Right now we just did PCA with two components, and set the first two components as the axis for the visualization.\n",
    "\n",
    "How much variation did these two components explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jO9DipP2e2LQ"
   },
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsqUhU_Se8EV"
   },
   "source": [
    "Here we can see the explained variance ratio for the first two components, the only two components in our PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us2S7hb0e1e-"
   },
   "source": [
    "What if we did PCA with more than 2 components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7gsIyr-Sc-K"
   },
   "outputs": [],
   "source": [
    "#keep more components and see how much variance they explain\n",
    "pca = PCA(n_components=10)  # Keeping 10 components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# variance Explained\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DybxtS92fDwh"
   },
   "source": [
    "---\n",
    "##### **Q3. Do you notice anything about how much variance is explained by the extra components? Is there a trend?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write Answer Here**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4GxdnksfKdb"
   },
   "source": [
    "Now let's take a look at the total cumulative variance explained by these components, when we have 10 components in the PCA.\n",
    "\n",
    "(Note that there are many ways to visualize explained variance of principal components in PCA and this is just one way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AbKFw_cSkJx"
   },
   "outputs": [],
   "source": [
    "#cumulative Variance Explained\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, 11), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(1, 11), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cbw564yTcmO"
   },
   "source": [
    "**Answer the question below.**\n",
    "\n",
    "##### **Q4. How does adding components change the cumulative explained variance?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write Answer Here**</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sskJrhqIakN8"
   },
   "source": [
    "*How do we pick how many principal components to explain a dataset?*\n",
    "\n",
    "Sometimes we pick the number of principal components to keep based on our goal of visualization: if we want to view a reduced dimensionality version of the dataset, we can only use maximum three dimensions. However, sometimes we want to use PCA for dimensionality reduction without a final goal of visualization. So how many components do we keep?\n",
    "\n",
    "We can take a look at:\n",
    "[Nature paper on PCA](https://www.nature.com/articles/s43586-022-00184-w)\n",
    "\n",
    "This paper gives us the \"elbow rule\", which suggests looking for a point where the explained variance by additional dimensions starts decreasing linearly. If we look at the cumulative explained variance plot earlier, we can draw a curve to fit the decline of each components contribution to explained variance. The point in that curve that is the 'elbow' of the curve, suggests where to cut off the number of components needed to explain the dimensionality of the data. If the first two components explain significantly more variance than the subsequent ones, this indicates that the dataset is essentially two-dimensional (almost all the variance can be explained by the first two principal components). Thus, a two-dimensional PCA effectively captures the major variance in data, with remaining dimensions likely representing noise or less significant variation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8lZc5UPbSNs"
   },
   "source": [
    "Here we draw a curve to find an \"elbow\" over our cumulative explained variance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn3Wbg2VaN-b"
   },
   "outputs": [],
   "source": [
    "# draw a curve that fits the tops of the histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(range(1, 11), explained_variance, alpha=0.5, align='center', label='Individual explained variance')\n",
    "plt.step(range(1, 11), cumulative_variance, where='mid', label='Cumulative explained variance')\n",
    "\n",
    "# plot curve over the individual explained variance\n",
    "plt.plot(range(1, 11), explained_variance, color='red', marker='o', linestyle='-',\n",
    "         label='Curve of Individual Variance')\n",
    "\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rRvmC3dfk4k"
   },
   "source": [
    "How much variance should be explained by the first principal components to be \"valid\"? i.e. how do we know PCA is actually explaining a good amount of variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6dBVZCwk8E9"
   },
   "source": [
    "*Answer*:\n",
    "\n",
    "If we are unable to explain a \"significant\" amount of variance in a \"reasonable\" number of components, maybe PCA is not the best tool to reduce the dimensionality of the data. In fact, if a significant amount of variance cannot be captured by the initial components, it may suggest that the data contains complex, non-linear relationships that PCA, a linear method, cannot adequately represent.\n",
    "\n",
    "What is \"significant\" variance and what is a \"reasonable\" number of components?\n",
    "The definition of \"significant\" variance and a \"reasonable\" number of components can vary depending on the specific context and domain of the data.\n",
    "\n",
    "*   General Rule of Thumb: a significant amount of variance is often considered to be around 70% or more of the total variance in the data\n",
    "*    One approach to determining a reasonable number of components is the elbow method, which involves plotting the explained variance against the number of components and looking for the \"elbow\" in the curve of the graph\n",
    "* Another approach to determining a \"reasonable\" number of components is based on visualization: reducing a dataset to three or fewer dimensions can enable visualization in 2D or 3D space, which might be desirable for certain analyses despite lower total explained variance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTSvvYkUdFgp"
   },
   "source": [
    "Let's take a look at how adding components changes the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC0fFvP1dCQ1"
   },
   "outputs": [],
   "source": [
    "# function to perform PCA and return explained variance\n",
    "def perform_pca(n_components, data):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(data)\n",
    "    return pca.explained_variance_ratio_, pca.components_\n",
    "\n",
    "# exploring the effect of different numbers of components\n",
    "components_to_try = [2, 5, 10, 15, 20, 25, 30]\n",
    "explained_variances = []\n",
    "for n in components_to_try:\n",
    "    explained_variance, _ = perform_pca(n, X_scaled)\n",
    "    explained_variances.append(sum(explained_variance))\n",
    "\n",
    "# plotting the total explained variance against the number of components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(components_to_try, explained_variances, marker='o')\n",
    "plt.title('Effect of Number of Components on Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Total Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxX7Zj7MSymW"
   },
   "source": [
    "### Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiOr6T68Jnyt"
   },
   "source": [
    "*How do we ensure our dimensionality reduction keeps the most important information?*\n",
    "\n",
    "There are many techniques for dimensionality reduction that aim to preserve the most important information. In the case of PCA, we will be learning how to preserve the features that \"explain the most variance\". This means, we will be summarizing the data using whatever features explain the most differences in our dataset, which ideally represents the labels of the classification task.\n",
    "\n",
    "Usually in dimensionality reduction, when we transform higher-dimensional data (data with a lot of features) into a lower dimensional space (the summary), each dimension in the lower-dimensional space represents a combination of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSrUn5eDfeTv"
   },
   "source": [
    "When we transform higher-dimensional data (data with a lot of features) into a lower dimensional space (the summary), each dimension in the lower-dimensional space represents a combination of the original features.\n",
    "\n",
    "Let's took a look at what composition of features these components have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N-rJugYUBaY"
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "_, components = perform_pca(n_components, X_scaled)\n",
    "\n",
    "# create a DataFrame for better visualization\n",
    "components_df = pd.DataFrame(components, columns=feature_names)\n",
    "\n",
    "# visualize the composition of the first few components\n",
    "plt.figure(figsize=(14, 8))\n",
    "for i in range(5):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    components_df.loc[i].plot(kind='bar')\n",
    "    plt.title(f'Component {i+1}')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wjQBe2eUHay"
   },
   "source": [
    "Intuitively, a positive weight in a component means that the feature contributes to that principal component.\n",
    "\n",
    "A negative weight in a PCA component means that the feature inversely correlates with that principal component. Each principal component is a linear combination of the original features, and the sign (positive or negative) indicates the direction of the correlation between the component and the feature. Negative weights suggest that as the feature value increases, the data points move in the opposite direction along that principal component axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTpwtZlYSujx"
   },
   "outputs": [],
   "source": [
    "# look at the composition of the first two principal components.\n",
    "pca_components = pd.DataFrame(pca.components_, columns=feature_names)\n",
    "print(pca_components.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQD9ux_jUQ3f"
   },
   "source": [
    "The PCA component weights reveal the relative importance of each feature in the dataset's variance. For instance, if texture has a high weight in the first principal component, it suggests that texture variation significantly contributes to distinguishing between different observations in the dataset. This might reflect the biological reality where texture variations in histopathology images are crucial for distinguishing between benign and malignant breast cancer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0fouzrTiEsN"
   },
   "source": [
    "\n",
    "\n",
    "## Conclusion\n",
    "PCA is a powerful tool for dimensionality reduction in high-dimensional datasets. Each component of PCA is a weighted combination of the features in the dataset and represent the most significant patterns or gradients of variation in the data. Often, these components may also capture important variations that well seperate points by some target value, making PCA a tool for effective visualization and generating useful insights into the data's underlying structure.\n",
    "\n",
    "As a reminder, some good practices for using PCA include:\n",
    "1. Scale your data before running PCA!\n",
    "2. Use the explained variance ratio to determine how many components to retain (usually we want above 70%).\n",
    "\n",
    "Despite the benefits, one must also be careful regarding the interpretation of PCA. While the components discovered by PCA are informative and useful in many cases, it is also true that many real-world datasets contain *non-linear* patterns that PCA is unable to capture. For these, we need more advanced algorithms that we will cover in Week 3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB5q9YeXOFfe"
   },
   "source": [
    "## Graded Exercises (5 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlnTYp6RL9VD"
   },
   "source": [
    "##### **GQ1. (1 marks) Diabetes Data PCA**\n",
    "> In the cell block below, we have loaded in a new dataset, the Diabetes Dataset, and printed out the description. Run PCA on this dataset and plot out the first two components of the PCA. Color the points by `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQeC3xLWMAAR"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the diabetes dataset\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq5wI87gNDb_"
   },
   "outputs": [],
   "source": [
    "### Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1adAE3_NFV8"
   },
   "source": [
    "---\n",
    "##### **GQ2. (2 marks) For the PCA you ran above, what is the explained variance of the first two components? Based on this, do you think two components is sufficient to capture the true structures of the data? Justify your answer.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6OXsDJ1NPtO"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6oS9o-8D_Kp"
   },
   "source": [
    "Write Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xAj9Fu_NQ_o"
   },
   "source": [
    "---\n",
    "##### **GQ3. (2 marks) Visualize the composition for the first two components. Which features are important for these components? Looking at the PCA components, do the important features make sense based on the value of y above? Justify your answer.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-VtNW6zN-w8"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpVnzTv_xBdo"
   },
   "source": [
    "## Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnPSY1vqxBdo"
   },
   "source": [
    "### Data Reconstruction from PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJyFSe8LV-fn"
   },
   "source": [
    "In this section, we delve into the concept of data reconstruction from PCA-transformed data, aiming to solidify our understanding of the dimensionality reduction process and its implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0GG2VlYS2Cy"
   },
   "outputs": [],
   "source": [
    "# reconstruct data from the reduced dataset (using inverse transformation)\n",
    "X_reconstructed = pca.inverse_transform(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zal7Ytn7V_rJ"
   },
   "source": [
    "Dimensionality reduction through PCA simplifies the dataset by projecting it onto a lower-dimensional space defined by the principal components. This process, while beneficial for reducing complexity and computational costs, entails a loss of information. By reconstructing the original data from its PCA-transformed version, we can visually and quantitatively assess what \"information loss\" actually means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okvirTJMS8Ku"
   },
   "source": [
    "### Compare Original Data and Reconstructed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i12QnTM7WRn0"
   },
   "source": [
    "Reconstruction involves reversing the PCA transformation, which provides us with a version of our dataset that retains only the variance captured by the selected principal components. Comparing this reconstructed dataset to our original dataset allows us to directly observe the consequences of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VdHKrVtS4cM"
   },
   "outputs": [],
   "source": [
    "# compare the reconstructed dataset to the original dataset\n",
    "# This can give us an idea of how much information we've managed to preserve or lose.\n",
    "error = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)\n",
    "print(f\"Mean reconstruction error: {np.mean(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afavwhcsTIgC"
   },
   "source": [
    "### Reconstruct with less components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gZvFzOxWio4"
   },
   "source": [
    "Let's take a look at what happens when we do PCA with less components. We can see how much information is lost in the reconstruction, or how much worse our reconstruction error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3iZqRAXTIF5"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)  # Keeping 2 components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_reconstructed = pca.inverse_transform(X_pca)\n",
    "error = np.mean((X_scaled - X_reconstructed) ** 2, axis=1)\n",
    "print(f\"Mean reconstruction error: {np.mean(error)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N24nmDdWw2Q"
   },
   "source": [
    " How does the reconstruction error affect our confidence in the PCA-transformed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAGjq0aMWsyp"
   },
   "source": [
    "*Answer*:\n",
    "\n",
    "The reconstruction error directly impacts our confidence in the PCA-transformed data. A lower reconstruction error indicates that the PCA-transformed data retains a significant portion of the original dataset's variance, meaning we can trust the reduced dataset to accurately represent the original data's structure and patterns. Conversely, a high reconstruction error suggests that much of the original data's information is lost in the transformation, potentially leading to inaccurate analyses or conclusions based on the PCA-transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx5R4hTPXHhD"
   },
   "source": [
    "Bonus: Can you think of any datasets for which PCA might not be the ideal method for dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z_OSYjkXQsS"
   },
   "source": [
    "*Answer*:\n",
    "\n",
    "\n",
    "\n",
    "* Non-Linear Data Structures: Remember when we couldn't explain a \"significant\" amount of variance in a \"reasonable\" number of components using PCA? PCA is fundamentally a linear method: it assumes that the principal components can linearly capture the dataset's variance. If the data contains complex, non-linear relationships, PCA may fail to identify these patterns, and techniques designed to capture non-linearity (like t-SNE or UMAP) might be more appropriate.\n",
    "* Interpretable Component Requirements: If the application requires easily interpretable components, PCA might not be ideal. PCA components are linear combinations of all original features, which can be challenging to interpret, especially in domains where specific features have distinct meanings.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
